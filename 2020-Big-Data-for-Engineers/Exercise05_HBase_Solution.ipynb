{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# <center>Big Data For Engineers &ndash; Exercises</center>\n## <center>Spring 2020 &ndash; Week 5 &ndash; ETH Zurich</center>\n## <center>HBase</center>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Overiew of this exercise sheet\nThis exercise consists of two main parts: \n* Hands-on practice with your own HBase cluster running in Azure.\n* Theory exercises on the architecture of HBase."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Exercise 1 &mdash; Creating and using an HBase cluster"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "It's time to touch HBase! You will create, fill with data, and query an HBase cluster running on Azure."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Do the following to set up an HBase cluster:\n\n**Important:** we want you to use a small but real cluster for running HBase rather than a single machine. But, these clusters burn Azure credit very quickly&mdash;the cheapest configuration consumes roughly **2 CHF per hour**, which is a lot relative to your overall credit&mdash;so it is very important for you to **delete your cluster once you are done.** Luckily, it is possible to keep your data intact when you delete a cluster, and see it again when you recreate it; we will touch upon this in the process. Now, let's start. Those steps are very similar to the HDFS cluster we create on week 3.\n\n1. In Azure portal click the **\"Create a resource\"** button on the left, type **\"hdinsight\"** in the search box, and select **\"Azure HDInsight\"** and click **\"Create\"**. HDInsight is Microsoft's cloud service which wraps Hadoop, HBase, Spark and other Big Data technologies; read more [here](https://azure.microsoft.com/en-us/services/hdinsight/).\n<img src=\"https://bigdataforeng2020.blob.core.windows.net/exercise05/HBase_1_Create_HDInsight.png\" width=\"700\">\n\n1. In the **\"Basics\"** tab, choose a subscription and **create a new resource group** (say \"exercise 05\").\n<img src=\"https://bigdataforeng2020.blob.core.windows.net/exercise05/HBase_2_Subscription_Resource_Group.png\" width=\"700\">\n\n1. Name your HBase cluster and select the region to be **\"(Europe) West Europe\"**. Choose **\"HBase\"** for cluster type and use **version 1.1.2**. Then setup the cluster login username and password, as well as the SSH username.\n<img src=\"https://bigdataforeng2020.blob.core.windows.net/exercise05/HBase_3_Cluster_Type.png\" width=\"700\">\n<img src=\"https://bigdataforeng2020.blob.core.windows.net/exercise05/HBase_4_Username.png\" width=\"700\">\n\n1. Next, we need to configure the **\"Storage\"** tab. The canonical way would be to use an HDFS cluster as a storage layer for an HBase cluster, but we will be using the Blob service of Windows Azure Storage for this purpose. This has a significant advantage of allowing you to delete your HBase cluster without losing the data: you can recreate the cluster using the same Azure Storage Account and the same container and you will see the same data. This is useful, for example, if you don't have time to finish this exercise in one sitting: you can just delete your cluster, recreate it later, and continue your work. Azure storage is selected by default (see the screenshot).  \nTo setup your HBase cluster for the first time, in **\"Primary storage account\"** click **\"Create new\"** and specify a name. Leave everything else as it is and click \"Next\".\n<img src=\"https://bigdataforeng2020.blob.core.windows.net/exercise05/HBase_5_Storage.png\" width=\"700\">\n**Important**: if you are recreating your HBase cluster and want to see the existing data, then choose **\"Select existing\"** and set the container name to the one that you used last time&mdash;by default Azure generates a new container name every time you create a cluster, which then points to a different container. The container name can be found in \"Storage account - containers\".\n<img src=\"https://bigdataforeng2020.blob.core.windows.net/exercise05/HBase_11_Container_name.png\" width=\"700\">\n<img src=\"https://bigdataforeng2020.blob.core.windows.net/exercise05/HBase_12_Reuse_storage.png\" width=\"700\">\n\n1. In the \"Security + networking\" tab do not choose anything, just click \"Next: Configuration + pricing\".\n\n1. Now we need to choose the configuration of the nodes in our HBase cluster. It will be enough to have **only 2 RegionServers** (see the screenshot). As for the node size, let us be wise and select the economical option: click on \"Region node size\" and choose **\"D3 V2\"**; do the same for the Head nodes; the \"Zookeeper\" nodes should have **\"A4 v2\"** selected by default (Zookeeper is a [distributed coordination service](http://zookeeper.apache.org/) used by HBase). Click **\"Review + create\"**.\n<img src=\"https://bigdataforeng2020.blob.core.windows.net/exercise05/HBase_6_Node_size.png\" width=\"700\">\n\n1. In the last step, \"Summary\", check if the settings are as you intend. These clusters are expensive, so it is worth checking the price estimate at this step: for me it is 1.90 CHF/hour; if your price is larger than this, check your node sizes and counts. When done, initiate the cluster creation by clicking \"Create\". The process will take time, around 15&mdash;25 minutes."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Accessing your cluster\nWe will interact with the HBase cluster through the command-line interface of HBase. For this, you will need to run the `ssh` program in a terminal in order to connect to your cluster. This process is the same as in week03's HDFS exercise, but we will repeat the instructions here for convenience.\n\nThere are three options of how you can do this:\n1. **On your own machine** you can just use a normal terminal if you have `ssh` installed. Linux usually has it, as does MacOS. Windows doesn't have it by default (maybe Windows 10 does?), but Windows users can use one of the browser-based options, which are described next, and the other option is to install [PuTTY](http://www.putty.org/).\n1. **In your browser:**\n  1. Use the **Azure Cloud Shell**. Click on the Cloud Shell icon at the top of Azure Dashboard toolbar:\n  <img src=\"https://bigdataforeng2020.blob.core.windows.net/exercise05/HBase_7_Azure_shell.png\" width=\"700\">\n  It will ask you to choose between `Bash` and `PowerShell`; select `Bash`. Then it will request your approval for creating a Storage Account required for the shell; agree to it. The shell will then appear at the bottom of the page.\n  1. Use a **terminal on Jupyter**. In your [notebooks.azure.com](https://notebooks.azure.com) tab, click **\"My Projects\"** at the top of the page. Then, **select one of your projects** and **click \"Terminal\"**. A shell will be opened in a new page.\n  <img src=\"https://bigdataforeng2020.blob.core.windows.net/exercise05/HBase_8_Jupyter_shell.png\" width=\"700\">\n\nIn your terminal of choice, run the following: \n\n`ssh <ssh_user_name>@<cluster_name>-ssh.azurehdinsight.net`\n\n![](https://bigdataforeng.blob.core.windows.net/ex05/ssh_edited.png)\n\nIn this command, `<ssh_user_name>` is the \"ssh username\" that you have chosen when creating the HBase cluster, and `<cluster_name>` also comes from that form. Note that the cluster name has to be suffixed with `-ssh`. This command with everything filled-in is also available for copying on **the Azure page of your HBase cluster**, if you click **\"SSH + Cluster login\"** and **choose the proper hostname**.\n<img src=\"https://bigdataforeng2020.blob.core.windows.net/exercise05/HBase_10_Login_command.png\" width=\"700\">\n\nIf after running the `ssh` command you see a message similar to this:\n```\nWelcome to HBase on HDInsight.\n\nLast login: Sat Oct 14 15:56:56 2017 from 180.220.17.157\nTo run a command as administrator (user \"root\"), use \"sudo <command>\".\nSee \"man sudo_root\" for details.\n\n<ssh_user_name>@hn0-cluster:~$\n```\nthen you have successfully connected to your HBase cluster. Here is an example of how it looks like on the Azure cloud terminal:\n<img src=\"https://bigdataforeng2020.blob.core.windows.net/exercise05/HBase_9_Login.png\" width=\"700\">"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Troubleshooting\nSome issues may arise while creating your HBase cluster. Here are some common issues that we experienced:\n1. *StorageAccountAlreadyExists* : Make sure to use a **unique name** while creating a new storage account. The portal does not check for this while in the creation panel but only on validation and an error will arise. This also holds for cluster names.\n1. *The ssh connection does not work* : Use the password that you provided at creation. If you can't retrieve it, you can reset the password in the \"SSH + Cluster login\" panel of your Hbase cluster. Also if you are recreating a new cluster, use a different cluster name as your past created cluster. Otherwise, this may create a conflict in your local *known_hosts* configuration file.\n\nYou can find more information about deployement errors on [this page](https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-manager-common-deployment-errors)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Interact with your HBase cluster using the shell\n\nIn this task we will go through some basic HBase commands, in preparation for the next exercise where we will import a big dataset and run queries against it."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Open the HBase shell\nOpen the HBase shell by running the following command:\n\n**`hbase shell`**"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create a table\nLet's say we want to create an HBase table that will store sentences adhering to the structure subject-verb-object (e.g., \"I eat mangoes\", \"She writes books\") in different languages. Here is a schema that we may use:\n\nTable name = `sentences`\n* Column family: `words`\n  * column: `subject`\n  * column: `verb`\n  * column: `object`\n* Column family: `info`\n  * column: `language`\n\nWith the following command we can create such a table (a description of HBase shell commands is available [here](https://learnhbase.wordpress.com/2013/03/02/hbase-shell-commands/)):\n\n**`create 'sentences', 'words', 'info'`**\n\nYou can see the schema of the table with this command:\n\n**`describe 'sentences'`**"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Update and query the table\nLet's insert some sentences into our table. We will put data cell by cell with the command  \n`put <table>, <rowId>, <columnFamily:columnQualifier>, <value>`:\n\n**`put 'sentences', 'row1', 'words:subject', 'I'`**\n\n**`put 'sentences', 'row1', 'words:verb', 'drink'`**\n\n**`put 'sentences', 'row1', 'words:object', 'coffee'`**\n\nNow, let's try to query this sentence from the table, which we can do with the command  \n`get <table>, <rowId>`:\n\n**`get 'sentences', 'row1'`**\n\nYou should see output similar to this:\n\n```\nCOLUMN                          CELL\n\n words:object                   timestamp=1555998158489, value=coffee\n\n words:subject                  timestamp=1555998139704, value=I\n\n words:verb                     timestamp=1555998148303, value=drink\n\n3 row(s) in 0.0540 seconds\n```\n\nAs you can see, HBase shell returns data as key-value pairs rather than as rows literally. You may also notice that the lines are **lexicographically sorted** by the key, which is why \"subject\" appears after \"object\" in the list.\n\nI don't know how about you, but I like tea more than coffee, so let me update our sentence...\n\n**`put 'sentences', 'row1', 'words:object', 'tea'`**\n\nAs you can see, we are using the same `put` command to **update** a cell. But remember that HBase does not actually update cells in place&mdash;it just inserts new versions instead. If you now run the query again, you will see the new data:\n\n**`get 'sentences', 'row1'`**\n\nreturns:\n\n```\nCOLUMN                          CELL\n\n words:object                   timestamp=1555998793452, value=tea\n\n words:subject                  timestamp=1555998139704, value=I\n\n words:verb                     timestamp=1555998148303, value=drink\n\n3 row(s) in 0.0470 seconds\n```"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Scan the table\nWe actually wanted to store sentences in different languages, so let's first set the language for the existing one:\n\n**`put 'sentences', 'row1', 'info:language', 'English'`**\n\nNote that we are now inserting a value into a different column family but for the same row. Verify with a `get` that this took effect. \n\nNow, let's add a sentence in another language (note that we are using another rowID now&mdash;`row2`):\n\n**`put 'sentences', 'row2', 'words:subject', 'Ich'`**\n\n**`put 'sentences', 'row2', 'words:verb', 'trinke'`**\n\n**`put 'sentences', 'row2', 'words:object', 'Wasser'`**\n\n**`put 'sentences', 'row2', 'info:language', 'Deutsch'`**\n\nLet's check that we indeed have 2 rows now:\n\n**`count 'sentences'`**\n\nNow, let's query all rows from the table:\n\n**`scan 'sentences'`**\n\nThis, indeed, returns all two rows, in key-value format as before.\n\nIt is, of course, possible to do some filtering in queries:\n\n*  **`scan 'sentences', {FILTER => \"ValueFilter(=, 'binary:English')\"}`** will find all cells with the value \"English\".\n\n*  **`scan 'sentences', {COLUMNS => 'words:subject', FILTER => \"ValueFilter(=, 'substring:I')\"}`** will find all cells in the column `words:subject` whose value contains a substring \"I\".\n\n*  **`scan 'sentences', {COLUMNS => 'words:object', ROWPREFIXFILTER => 'row'}`** will find all cells in the column `words:object` whose row key starts with the prefix `row`."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Add a new column to the table\nWhat if we want to store a sentence that also contains an adjective, in addition to the subject, verb, and object? This is not a problem with HBase, because we can create new columns inside **existing** column families on the fly:\n\n**`put 'sentences', 'row3', 'words:subject', 'Grandma'`**\n\n**`put 'sentences', 'row3', 'words:verb', 'bakes'`**\n\n**`put 'sentences', 'row3', 'words:adjective', 'delicious'`**\n\n**`put 'sentences', 'row3', 'words:object', 'cakes'`**\n\nThis row now has more columns in the `words` column family than others:\n\n**`get 'sentences', 'row3'`**\n\nWe can also add new columns to existing rows:\n\n**`put 'sentences', 'row1', 'words:adjective', 'hot'`**\n\n**`get 'sentences', 'row1'`**"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This was a quick overview of HBase shell commands. In the following task we will import a real, sizeable dataset (a subset of Wikipedia) and see how HBase will handle it."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Important: if you do not plan to do the next section right now, please delete your cluster and just recreate it when you need it again."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Exercise 2 &mdash; The Wikipedia dataset"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Download the dataset\nIn this task we will see how HBase will handle a large dataset and see how a choice of column families may affect performance.\n\nLet's begin. First, SSH to your cluster as in the previous task:\n\n**`ssh <ssh_user_name>@<cluster_name>-ssh.azurehdinsight.net`**\n\nNote that if you are still in the Hbase shell from the last exercise, you can can quit by entering `exit`.\n\nDownload the compressed dataset:\n\n**`wget https://bigdataforeng2020.blob.core.windows.net/exercise05/wikibig.tar.gz`**\n\nDecompress it:\n\n**`tar xvf wikibig.tar.gz`**\n\nThe dataset comprises approximately 100,000 articles of the English Wikipedia. You will see four files: \n\n| File | What's inside |\n|:------|:---------------|\n|`text.csv`| Text of the article. |\n|`author.csv`| The username of the latest version's author.|\n|`comment.csv`| Comment that the author left about the last change to the article.|\n|`timestamp.csv`| When that last change was made. Note that this \"timestamp\" is different from HBase's \"timestamp\". |\n\nThe files are in a comma-separated \"`key,value`\" format in which `key` is the article title."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Upload the dataset to HDFS\nBefore we can insert the data into HBase, we need to upload it into \"HDFS\" (for our HDInsight cluster it is actually Azure Blobs). Note that uploading `text.csv` can take a couple of minutes:\n\n**`hdfs dfs -put author.csv /tmp/`**\n\n**`hdfs dfs -put comment.csv /tmp/`**\n\n**`hdfs dfs -put timestamp.csv /tmp/`**\n\n**`hdfs dfs -put text.csv /tmp/`**"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Import the dataset to HBase\nLet us create the schemas in HBase now\n\n**`hbase shell`**\n\nIn order to see what difference column family choice can make, we need to create two different tables, each with a different schema, which we will populate with the same data. One of them will have a single column family (which we name `data`), into which all the four columns (`author`, `timestamp`, `comment`, `text`) will go:\n\n**`create 'wiki_1colfam', 'data'`**\n\nThe other table will have two column families&mdash;one for **metadata** (`author`, `timestamp`, `comment`) and the other for the article **content** (article text is, of course, larger in size than the metadata):\n\n**`create 'wiki_2colfams', 'metadata', 'content'`**\n\nIn both tables, **the row key is the name of the Wikipedia article**.\n\nAfter the two tables are created, we need to **exit the HBase shell** to return back to the head node's shell:\n\n**`exit`**\n\nNow we need to populate both tables with data. We will use the [ImportTsv](https://hbase.apache.org/book.html#importtsv) utility of HBase.\n\nPopulate the table `'wiki_1colfam'` by running the following four commands, each of which uploads one column. Note that these commands print a lot of messages, but they are mostly informational with an occasional non-critical warning; unless something goes wrong, of course :) The commands will also report some \"Bad Lines\", but you can safely ignore this&mdash;some lines may contain illegal characters and be dropped, but most of the data is in good shape.\n\n**`hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=, -Dimporttsv.columns=\"HBASE_ROW_KEY, data:author\" wiki_1colfam wasbs:///tmp/author.csv`**\n\n**`hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=, -Dimporttsv.columns=\"HBASE_ROW_KEY, data:comment\" wiki_1colfam wasbs:///tmp/comment.csv`**\n\n**`hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=, -Dimporttsv.columns=\"HBASE_ROW_KEY, data:timestamp\" wiki_1colfam wasbs:///tmp/timestamp.csv`**\n\n**`hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=, -Dimporttsv.columns=\"HBASE_ROW_KEY, data:text\" wiki_1colfam wasbs:///tmp/text.csv`**\n\nThe last command imports the biggest column, `text`, so it will take time; up to a couple of minutes.\n\nNow we need to populate the other table, `wiki_2colfams`. We will use the same four commands, but notice that we use a different table name and that the `text` column now gets its own column family.\n\n**`hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=, -Dimporttsv.columns=\"HBASE_ROW_KEY, metadata:author\" wiki_2colfams wasbs:///tmp/author.csv`**\n\n**`hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=, -Dimporttsv.columns=\"HBASE_ROW_KEY, metadata:comment\" wiki_2colfams wasbs:///tmp/comment.csv`**\n\n**`hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=, -Dimporttsv.columns=\"HBASE_ROW_KEY, metadata:timestamp\" wiki_2colfams wasbs:///tmp/timestamp.csv`**\n\n**`hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=, -Dimporttsv.columns=\"HBASE_ROW_KEY, content:text\" wiki_2colfams wasbs:///tmp/text.csv`**"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Tasks to do\n\n1. Write the following queries using the HBase shell:\n  1. Print the title and the author's name for each article whose title starts with '`Albert`'.\n  1. Print the title and the author's name for each article whose author's name contains the substring '`tom`'.\n1. Execute your queries on the two tables (more than once) and observe the query execution times\n1. What are the advantages and disadvantages of pure row stores?\n1. What are the advantages and disadvantages of pure column stores?\n1. What are the advantages and disadvantages of wide column stores?\n1. What are the advantages and disadvantages of denormalization?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Solutions\n\n1. The two queries:\n  1. All article titles and author names where the article title starts with 'Albert':\n    1. `scan 'wiki_1colfam', {COLUMNS => 'data:author', ROWPREFIXFILTER => 'Albert'}`\n    1. `scan 'wiki_2colfams', {COLUMNS => 'metadata:author', ROWPREFIXFILTER => 'Albert'}`\n  1. All article titles and author names where the author name contains the substring '`tom`'\n    1. `scan 'wiki_1colfam', {COLUMNS => 'data:author', FILTER => \"ValueFilter(=, 'substring:tom')\"}`\n    1. `scan 'wiki_2colfams', {COLUMNS => 'metadata:author', FILTER => \"ValueFilter(=, 'substring:tom')\"}`\n1. Execution times\n  1. Queries with `ROWPREFIXFILTER` should be quick for both tables, because the filter is applied to the row key rather than to the contents of columns. But even this query could be slower on the table with a single column family, *especially on the first invocation of the query*, because more unrelated data has to be loaded to extract the author name.\n  1. The query which searches for a substring in author name takes longer for the table with one column family than for the table with a separate column family for the metadata. HBase stores columns of a single family together and it has to load them together too. So, if for applying a filter to the author column we also have to load the full text of the article (as is the case with just one column family), the operation will take longer than if we don't have to (as is the case with a separate column family for the article text and for metadata).\n  Subsequent invocations of the same command take less time due to caching.\n1. **Pure row store:**\n  1. Advantages:\n    1. Good for workloads with point lookups and updates. Retrieving (updating) a single row is efficient as the row is colocated\n  1. Disadvantages:\n    1. Scans are more expensive (whole row is always retrieved)\n1. **Pure column store:**\n  1. Advantages:\n    1. Scans are very efficient (only specific columns can be retrieved)\n  1. Disadvantages:\n    1. To retrieve (or update) a whole row, many random accesses need to be performed\n1. **Wide column store:**\n  1. Advantages:\n    1. Column families offer a 'middle ground' between pure row- and column-oriented storages.  Columns frequently accessed together can be colocated, very wide columns (affecting scan speed) can be isolated into separate column families\n    1. Flexible schema (column names stored for each row) offer flexibility for cases where schema is not known upfront (or in cases of sparse columns)\n  1. Disadvantages\n    1. Performance penalties, point lookups not as fast as pure row store, scans not as fast as pure column store\n    1. The key holds the row key, the column family name, the column qualifier, etc. This can result in a considerable storage overhead if the values that we want to store are small in comparison the key.\n1. **Denormalization:**\n  1. Advantages:\n    1. All operations are either scans or point lookups. No need for expensive joining of multiple relations (all data is colocated or easily mapped)\n  1. Disadvantages:\n    1. It is difficult to enforce (maintain) consistency in cases of updates\n    1. Storage (memory) overhead, due to duplicated data\n    1. Scan processing can be more expensive"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Important: you may delete your HBase cluster now. \nThe next exercise will focus on HBase's architecture."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Exercise 3 &mdash; Architecture of HBase\n\nIn the previous tasks, we have seen HBase in action. Let us now take a look at the internal architecture of HBase. You may want to consult the lecture slides when solving these tasks."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Task 3.1 &mdash; Inside a RegionServer"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this exercise you will see how a RegionServer in HBase would execute a query.\n\nImagine that we have an HBase table called '`phrases`', which has the following schema:\n\n* Column family: `words`\n  * column: A\n  * column: B\n  * column: C\n  * (potentially also columns D, E, F, etc.)\n\nThus, the table has only one column family. Each column in this family holds one word.\n\nRecall from the lecture slides that keys in HBase have the following structure:\n<img src=\"https://bigdataforeng2020.blob.core.windows.net/exercise05/hbase-key-structure.png\" width=\"70%\">\n\nWe need make certain simplifications to the format of keys to avoid excessive clutter in this exercise. Since the table in this exercise has only one column family, we will omit it from the key and will only specify the column name (A,B,C, ...). We will also omit the length fields and the \"key type\" field. The timestamp field in this exercise will contain integers from 1 to 10, where in reality it would contain the number of milliseconds since an event in the long past. **Thus, keys as will be used in this exercise consist of three fileds: row, column, timestamp.**"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Tasks to do\n\nState which Key-Value pairs will be returned by each of the following queries, given in HBase shell syntax which you have already seen in the first exercise. Assume that the HBase instance is configured to return **only the latest version** of a cell and that the columns are returned in *lexicographic order*.\n\n1. `get 'phrases', '209'`\n1. `get 'phrases', '491'`\n1. `get 'phrases', '900'`\n1. `get 'phrases', '743'`\n1. `get 'phrases', '145'`\n\nTo answer this question, use the diagram below, which represents the state of a RegionServer responsible for the row region in the range of row IDs 100&ndash;999, which is the region into which all these queries happen to fall. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "A larger, zoomable, PDF version of this diagram is available [here](https://bigdataforeng2020.blob.core.windows.net/exercise05/HBase_Architecture_Queries.pdf)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![Overall_Instance1](https://bigdataforeng2020.blob.core.windows.net/exercise05/HBase_Architecture_Queries.png)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Solution to the Task 3.1\n\n1. get 'phrases', '209' \n\n| Row | Column | Timestamp | Value | Where it came from |\n|:-----:|:-----:|:-----:|:-------|:--------------------:|\n|209|A|5|oranges|HFile 1|\n|209|B|2|taste|HFile 3|\n|209|C|8|nice|HFile 2|\n\nNothing special here: for that rowID we just return all found key-values. All values come from HFiles.\n\n1. get 'phrases, '491'\n\n| Row | Column | Timestamp | Value | Where it came from |\n|:-----:|:-----:|:-----:|:-------|:--------------------:|\n|491|A|6|books|HFile 2|\n|491|B|2|record|MemStore|\n|491|C|1|wisdom|HFile 1|\n\nIn this case, one value came from the MemStore.\n\n1. get 'phrases', '900'\n\n| Row | Column | Timestamp | Value | Where it came from |\n|:-----:|:-----:|:-----:|:-------|:--------------------:|\n|900|C|9|confidence|HFile 2|\n|900|D|3|helps|HFile 2|\n\nThis example lacks values for columns \"A\" and \"B\", and this is perfectly fine with HBase. Also, both values come from the same HFile.\n\n1. get 'phrases', '743'\n\n| Row | Column | Timestamp | Value | Where it came from |\n|:-----:|:-----:|:-----:|:-------|:--------------------:|\n|743|A|3|his|HFile 1|\n|743|B|6|brother|HFile 3|\n|743|C|3|likes|HFile 2|\n|743|D|3|apricots|MemStore|\n\nIn this example, versions matter.\nThe value `743,B,5,father` from HFile 1 gets superceded by `743,B,6,brother` from HFile 3, which has a higher version number.\nThe same happens to value `743,D,2,apples` from HFile 1, which gets superceded by `743,D,3,apricots` from MemStore.\n\n1. get 'phrases', '145'\n\nThe rowID `145` does not exist, so nothing is returned."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Own exploration\n## Building an HFile index"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "When performing a get, the RegionServer needs to check its MemStore and all HFiles (unless the Bloom filter returns negative) for the existence of the requested key. In order to avoid scanning HFiles entirely, HBase uses index structures to quickly skip to the position of the *HBase block* which may hold the requested key.\n\nBy default, each *HBase block* is 64KB (configurable) in size and always contains whole key-value pairs, so, if a block needs more than 64KB to avoid splitting a key-value pair, it will just grow.\n\nIn this task, you will be building the index of an HFile. __For the purpose of this exercise__, assume that each HBase block is 40 bytes long, and each character in keys and values is worth 1 byte: for example, the first key-value pair in the diagram below is worth $3 + 1 + 1 + 6 = 11$ bytes. Below this diagram you will find a table for you to fill in."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<img src=\"https://bigdataforeng2020.blob.core.windows.net/exercise05/HBase_HFile_Index_Task.png\" width=\"50%\">"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Based on the contents of the HFile above, you need to **populate the index**, following the approach described in the lecture slides. Use the following table (again, you can edit it by double-clicking). Use as many or as few rows as you need.\n\n| RowId | Column | Version |\n|-------|--------|---------|\n|       |        |         |\n|       |        |         |\n|       |        |         |\n|       |        |         |\n|       |        |         |\n|       |        |         |\n|       |        |         |\n|       |        |         |\n|       |        |         |\n|       |        |         |"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Solution\n<img src=\"https://bigdataforeng2020.blob.core.windows.net/exercise05/HBase_HFile_Index_Solution.png\" width=\"700\">"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}