{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Big Data for Engineers &ndash; Exercises</center>\n",
    "## <center>Spring 2020 &ndash; Week 3 &ndash; ETH Zurich</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This week we will cover mostly theoretical aspects of Hadoop and HDFS and we will discuss advantages and limitations of different storage models.\n",
    "The mandatory reading is [1] [Shvachko, K. et al. (2010). The Hadoop Distributed File System. In MSST.](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5496972)\n",
    "\n",
    "#### What is Hadoop?\n",
    "_\"Hadoop provides a **distributed file system** and a\n",
    "**framework for the analysis and transformation** of very **large**\n",
    "data sets using the MapReduce paradigm.\"_ [1] \n",
    "\n",
    "Several components are part of this framework. In this course you will study HDFS, MapReduce and HBase while this exercise focuses on HDFS and storage models.\n",
    "\n",
    "\n",
    "| *Component*                |*Description*  |*First developer*  |\n",
    "|----------------------------------------------|---|---|\n",
    "| **HDFS**                  |Distributed file system  |Yahoo!  |\n",
    "| **MapReduce**   |Distributed computation framework   |Yahoo!  |\n",
    "| **HBase**           | Column-oriented table service  |Powerset (Microsoft)  |\n",
    "| Pig  | Dataflow language and parallel execution framework  |Yahoo!   |\n",
    "| Hive            |Data warehouse infrastructure   |Facebook  |\n",
    "| ZooKeeper    |Distributed coordination service   |Yahoo!  |\n",
    "| Chukwa  |System for collecting management data   |Yahoo!  |\n",
    "| Avro                |Data serialization system   |Yahoo! + Cloudera  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Hadoop Distributed File System\n",
    "### 1.1 &ndash; State which of the following statements are true:\n",
    "\n",
    "1. The HDFS namespace is a hierarchy of files and directories.\n",
    "\n",
    "1. In HDFS, each block of the file is either 64 or 128 megabytes depending on the version and distribution of Hadoop in use, and this cannot be changed.\n",
    "\n",
    "1. A client wanting to write a file into HDFS, first contacts the NameNode, then sends the data to it. The NameNode will write the data into multiple DataNodes in a pipelined fashion. \n",
    "\n",
    "1. A DataNode may execute multiple application tasks for different clients concurrently.\n",
    "\n",
    "1. The cluster can have thousands of DataNodes and tens of thousands of HDFS clients per cluster.\n",
    "\n",
    "1. HDFS NameNodes keep the namespace in RAM.\n",
    "\n",
    "1. The locations of block replicas are part of the persistent checkpoint that the NameNode stores in its native file system. <font color='blue'>(more detail in [1])</font>\n",
    "\n",
    "1. If the block size is set to 64 megabytes, storing a file of 80 megabytes will actually require 128 megabytes of physical memory (2 blocks of 64 megabytes each). \n",
    "\n",
    "1. Every 6 hours the _NameNode_ asks the _DataNode_ for a complete report on the blocks stored on it.\n",
    "\n",
    "1. HDFS is optimised for latency. \n",
    "\n",
    "\n",
    "\n",
    "### 1.2 &ndash; Where is the information stored? For each of the following, say if the information is stored in the disk of the NameNode, in the disk of a DataNode, in disks of both of them, or in none of them.\n",
    "1. the list of blocks belonging to each file _[NameNode | DataNode]_\n",
    "\n",
    "1. the files containing the actual blocks of data _[NameNode | DataNode]_\n",
    "\n",
    "1. the block's metadata including checksum and generation stamp _[NameNode | DataNode]_\n",
    "\n",
    "1. the location of block replicas _[NameNode | DataNode]_ <font color='blue'>(more detail in [1])</font>\n",
    "\n",
    "\n",
    "\n",
    "### 1.3 &ndash; A typical filesystem block size is 4096 bytes. How large is a block in HDFS? List at least two advantages of such choice.\n",
    "\n",
    "\n",
    "\n",
    "### 1.4 &ndash; How does the hardware cost grow as function of the amount of data we need to store in a Distributed File System such as HDFS? Why?\n",
    "\n",
    "\n",
    "### 1.5 &ndash; Scalability, Durability and Performance on HDFS\n",
    "Explain how HDFS accomplishes the following requirements:\n",
    "\n",
    "1. Scalability\n",
    "\n",
    "1. Durability\n",
    "\n",
    "1. High sequential read/write performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. File I/O operations and replica management.\n",
    "\n",
    "\n",
    "### 2.1 &ndash; Replication policy\n",
    "Assume your HDFS cluster is made of 3 racks, each containing 3 DataNodes. Assume also the HDFS is configured to use a block size of 100 megabytes and that a client is connecting from outside the datacenter (therefore no DataNode is priviledged). \n",
    "\n",
    "1. The client uploads a file of 150 megabytes. Draw in the picture below a possible blocks configuration according to the default HDFS replica policy. How many replicas are there for each block? Where are these replicas stored?\n",
    "\n",
    "1. Can you find a with a different policy that, using the same number of replicas, improves the expected availability of a block? Does your solution show any drawbacks?\n",
    "\n",
    "1. Referring to the picture below, assume a block is stored in Node 3, as well as in Node 4 and Node 5. If this block of data has to be processed by a task running on Node 6, which of the three replicas will be actually read by Node 6? \n",
    "\n",
    "\n",
    "![Cluster](https://exerciseassets.blob.core.windows.net/exercise03/cluster.jpg)\n",
    "\n",
    "\n",
    "### 2.2 &ndash; File read and write data flow.\n",
    "To get an idea of how data flows between the client interacting with HDFS, consider a diagram below which shows main components of HDFS. \n",
    "\n",
    "<img src=\"https://exerciseassets.blob.core.windows.net/exercise03/clientHDFS.jpg\" style=\"height: 400px;\"/>\n",
    "\n",
    "1. Draw the main sequence of events when a client copies a file to HDFS.\n",
    "2. Draw the main sequence of events when a client reads a file from HDFS.\n",
    "3. Why do you think a client writes data directly to datanodes instead of sending it through the namenode?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Storage models\n",
    "Last week we delve into the key-value model (incl. object storage, e.g., Amazon S3/Azure Blob). This week, we can contrast the object storage in the key-value model with block storage, which is used for distributed file systems. \n",
    "\n",
    "\n",
    "### 3.1 &ndash; List two differences between Object Storage and Block Storage.\n",
    "\n",
    "\n",
    "### 3.2 &ndash; Compare Object Storage and Block Storage. For each of the following use cases, say which technology (object or block storage) better fits the requirements and briefly justify why.\n",
    "\n",
    "1. Store Netflix movie files in such a way they are accessible from many client applications at the same time _[Object storage | Block Storage ]_\n",
    "\n",
    "1. Store experimental and simulation data from CERN _[Object storage | Block Storage ]_\n",
    "\n",
    "1. Store the auto-backups of iPhone/Android devices _[Object storage | Block Storage ]_\n",
    "\n",
    "\n",
    "\n",
    "### 3.3 &ndash; Cost of Object Storage (Optional)\n",
    "Azure Object Storage offers different access tiers, which allow you to store blob object data in the most cost-effective manner. \n",
    "\n",
    "Imagine you want to have a copy of your important documents, photos and videos to both ensure durability and to share them accross your several devices. You need about 600GB of storage space. Two possibilities would be [hot and cool storage tiers on Azure Blob Storage](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers). \n",
    "\n",
    "1. Explain the diffrerence between hot and cool storages. \n",
    "\n",
    "1. Compare costs of cool and hot storages on [Azure Storage](https://azure.microsoft.com/en-us/pricing/details/storage/blobs/). Explain why prices are different. Which one would you choose? Why?\n",
    "\n",
    "1. Another possibile solution would be to pay for a [Dropbox Pro account](https://www.dropbox.com/business/plans-comparison). Compare costs of Dropbox and Azure storage. Which one is cheaper?\n",
    "\n",
    "1. What about buying an external hard drive? List two advantages and two disadvantages of this solution.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore on your own\n",
    "\n",
    "## 4. Run an HDFS cluster\n",
    "It's time to work on a real enviroment! You will now create an HDFS cluster running on Azure and fill it with data. The purpose of this exercise is to familiarise you with shell commands and the operations to administer an HDFS deployment.\n",
    "\n",
    "### 4.1 &ndash; Do the following to set up an HDFS cluster:\n",
    "\n",
    "**Important:** we want you to use a small but real cluster for running HDFS rather than a single machine. But, these clusters burn Azure credit very quickly&mdash;the cheapest configuration consumes roughly **1 CHF per hour**, which is a lot relative to your overall credit&mdash;so it is very important for you to _<font color='red'>**delete**</font>_ your cluster once you are done. Luckily, _it is possible to keep your data intact when you delete a cluster_ , and see it again when you recreate it; we will touch upon this in the process. Now, let's start ...\n",
    "\n",
    "\n",
    "1. Open the [Azure portal](https://portal.azure.com/) and click on the \"+ Create a Resource\" button on the left. Type \"hdinsight\" in the search box, and select \"HDInsight\". HDInsight is Microsoft's cloud service which wraps Hadoop, HBase, Spark and other Big Data technologies; read more [here](https://azure.microsoft.com/en-us/services/hdinsight/).\n",
    "![](https://bigdataforeng2020.blob.core.windows.net/exercise03/Azure_HDFS_1.png)\n",
    "\n",
    "\n",
    "2. Click on Create, fill in the form with cluster name, user names and passwords, and select \"Hadoop\" as the cluster type. Create a new resource group, e.g., \"exercise03\". Click \"Next\".\n",
    "\n",
    "\n",
    "![](https://bigdataforeng2020.blob.core.windows.net/exercise03/Azure_HDFS_2.png)\n",
    "\n",
    "3. In the next we need to configure the location to store all the cluster data. The canonical storage layer for an HDFS cluster uses the Blob service of Windows Azure Storage and the primary advantage is that it allows you to delete your HDFS cluster without losing the stored data: you can recreate the cluster using the same Azure Storage Account and the same container and you will see the same data. This is useful, for example, if you don't have time to finish this exercise in one sitting: you can just delete your cluster, recreate it later, and continue your work. Azure storage is selected by default (see the screenshot). In \"Select a Storage Account\" click \"Create new\" and specify a name. **Important: if you are recreating your HDFS cluster and want to see the existing data, then choose \"Select existing\" and set the container name to the one that you see in the \"Storage Accounts\" tab of Azure&mdash;by default Azure generates a new container name every time you create a cluster, which then points to a different container.** \n",
    "Leave everything else as it is and click \"Next\".\n",
    "4. In the \"Security + networking\" step do not choose anything and just click \"Next\".\n",
    "5. Now we need to choose the configuration of the nodes in our HDFS cluster. Nodes in HDIsight have similar names yet types, compared with the concepts of HDFS you have learned from the lecture. In this exercise, you will see _head node (aka namenode) and worker node (aka datanode)_. To read more, this blog on [\"Nodes in HDInsight\"](https://blogs.msdn.microsoft.com/azuredatalake/2017/03/10/nodes-in-hdinsight/) is helpful. It will be enough to have only 2 **worker** nodes (see the screenshot). As for the node size, let us be wise and select an economical option: from the drop-down menu choose the option \"**A4 - v2**\"(see the screenshot); do the same for the **Head** nodes. Click \"Next\".\n",
    "![](https://bigdataforeng2020.blob.core.windows.net/exercise03/Azure_HDFS_3.png)\n",
    "\n",
    "6. All the fields on the \"Tags\" step can be left as they are. Simply proceed by clicking \"Next\".\n",
    "7. In the last step, \"Review + create\", check if the settings are as you intend. These clusters are expensive, so it is worth checking the price estimate at this step: for me it is 0.87 CHF/hour; if your price is larger than this, check your node sizes and counts. When done, initiate the cluster creation by clicking \"Create\". The process will take time, around 15&mdash;25 minutes; in my own case it took 20 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 &ndash; Accessing your cluster\n",
    "The standard way to interact with an HDFS cluster is via the Java API or a command-line interface. We will use the latter and for this, you will need to run the `ssh` program in a terminal in order to connect to your cluster. There are three options of how you can do this:\n",
    "1. **On your own machine** you can just use a normal terminal if you have `ssh` installed. Linux usually has it, as does MacOS. Windows doesn't have it by default (powershell on Win10 does, though), but Windows users can use one of the browser-based options, which are described next, and the other option is to install [PuTTY](http://www.putty.org/).\n",
    "1. **In your browser, two possibilities:**\n",
    "  1. Use the **Azure Cloud Shell**. Click on the Cloud Shell icon at the top of Azure Dashboard toolbar:\n",
    "  ![](https://bigdataforeng2020.blob.core.windows.net/exercise03/Azure_HDFS_4.png)\n",
    "  Choose \"bash\". It will request your approval for creating a Storage Account required for the shell; choose the corresponding subscription and you can also create a new storage account or use an old storage account.\n",
    "  1. Use the built-in **terminal on Jupyter**. In your [notebooks.azure.com](https://notebooks.azure.com), find the terminal button on the right side, click on it. \n",
    "  ![](https://bigdataforeng2020.blob.core.windows.net/exercise03/Azure_HDFS_5.png)\n",
    "\n",
    "In your terminal of choice, run the following (this command with everything filled-in is also available on the Azure page of your HDFS cluster, if you click \"SSH + Cluster login\" and select the host name): \n",
    "<br>\n",
    "`ssh <ssh_user_name>@<cluster_name>-ssh.azurehdinsight.net`\n",
    "\n",
    "In this command, `<ssh_user_name>` is the \"ssh username\" that you have chosen in the first step of creating the HDFS cluster, and `<cluster_name>` also comes from that form. Note that the cluster name has to be suffixed with `-ssh`. \n",
    "\n",
    "\n",
    "If after running the `ssh` command you see a message similar to this:\n",
    "```\n",
    "Welcome to HDInsight.\n",
    "\n",
    "[...]\n",
    "\n",
    "To run a command as administrator (user \"root\"), use \"sudo <command>\".\n",
    "See \"man sudo_root\" for details.\n",
    "\n",
    "<ssh_user_name>@hn0-cluster:~$\n",
    "```\n",
    "then you have successfully connected to your HDFS cluster. Now proceed to the next task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 &ndash; Monitor the HDFS cluster\n",
    "In the above screenshot, we highlight the **Cluster Dashboards** powered by Ambari. The Apache Ambari project is aimed at making Hadoop management simpler by developing software for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its RESTful APIs ([source](https://ambari.apache.org/)).\n",
    "\n",
    "We utilize this monitoring dashboard to inspect the behaviors of our HDFS cluster. You will see prominent behavior changes (e.g., resource usage live for the nodes and network usage going up) esp. when uploading some new data. \n",
    "\n",
    "Click into the Ambari home. You can explore the following: \n",
    "1. An overview of the cluster (namenode, datanode)\n",
    "    - One namenode is active, the other one is standby\n",
    "<img src=\"https://bigdataforeng2020.blob.core.windows.net/exercise03/Azure_HDFS_6.png\" />\n",
    "    \n",
    "    \n",
    "2. Space utilization on HDFS and the node details (2 namenodes, 2 datanodes, 3 zookeeper nodes)\n",
    "    - The \"Zookeeper\" nodes are selected by default (Zookeeper is a [distributed coordination service](http://zookeeper.apache.org/) used by HDFS). By default, HDInsight provides 3 Zookeeper nodes. \n",
    "    - You can click on each box to see the operating details of each node.\n",
    "<img src=\"https://bigdataforeng2020.blob.core.windows.net/exercise03/Azure_HDFS_7.png\" />\n",
    "    \n",
    "    \n",
    "3. Under Hosts, you will find more details\n",
    "<img src=\"https://bigdataforeng2020.blob.core.windows.net/exercise03/Azure_HDFS_8.png\"/>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 &ndash; Upload a file into HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us go over some common [commands](https://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-common/FileSystemShell.html):\n",
    "```\n",
    " ls, cat, mkdir, cp, rm, rmdir, copyFromLocal, copyToLocal\n",
    "```\n",
    "A quick note: we will use the ```hadoop fs``` keywords as a prefix to the commands, but we could have used ```hdfs dfs``` keywords as well. The only difference is that ```hadoop fs``` is a more “generic” command that allows you to interact with multiple file systems including Hadoop (but also your local file system), whereas ```hdfs dfs``` is specific to HDFS. \n",
    "\n",
    "1. List the directories or files: ```ls``` \n",
    "    (some of these commands could be slow)\n",
    "    - For a file ls returns statistics on the file: ```$hadoop fs -ls /example/data/fruits.txt```\n",
    "    <br>This gives you the following:\n",
    "    ```-rw-r--r--   1 root supergroup         66 2020-03-01 17:42 /example/data/fruits.txt```\n",
    "    - For a directory it returns list of its direct children as in Unix: ```$hadoop fs -ls /example```\n",
    "    <br>This gives you the following:\n",
    "    \n",
    "    ```Found 2 items\n",
    "    drwxr-xr-x   - root supergroup          0 2020-03-01 17:43 /example/data\n",
    "    drwxr-xr-x   - root supergroup          0 2020-03-01 17:43 /example/jars\n",
    "    ```\n",
    "    <br>\n",
    "    - List files in a directory: ```$hadoop fs -ls /example/data/*```\n",
    "    - List all the directories in your HDFS: ```$hadoop fs -ls -R /```\n",
    "<br><br>\n",
    "2. Copies source paths to stdout: ```cat```\n",
    "    - Inspect the file content (**DO NOT** do it for a large file): \n",
    "    \n",
    "    ```$hadoop fs -cat /example/data/fruits.txt```\n",
    "<br><br>\n",
    "3. Copy files from source to destination: ```cp```\n",
    "    - \n",
    "    ```$hadoop fs -cp /example/data/fruits.txt /example/data/fruits-copy.txt```\n",
    "This shouldn't return anything.\n",
    "<br><br>\n",
    "4. Delete files: ```rm```\n",
    "    - \n",
    "    ```$hadoop fss -rm /example/data/fruits-copy.txt```<br>\n",
    "    This gives you:\n",
    "    ```Deleted /example/data/fruits-copy.txt```\n",
    "    \n",
    "    - When you then check ```$hadoop fs -ls /example/data/fruits-copy.txt```,\n",
    "    you will see the file does not exist anymore by getting this message: \n",
    "    ```ls: '/example/data/fruits-copy.txt': No such file or directory```\n",
    "<br><br>\n",
    "\n",
    "6. Creates directories: ```mkdir```\n",
    "    - We create a folder for our exercise by ```$hadoop fs -mkdir /ex03```\n",
    "<br><br>\n",
    "\n",
    "7. Copy a file from the file system where your terminal locates to HDFS: ```copyFromLocal```\n",
    "    - First we create a file on the file system where your terminal locates (can be local or on a cluster):\n",
    "    \n",
    "    ``` $ echo \"we create a file on the cluster(hn0) file system\" > cluster.txt```\n",
    "    \n",
    "    ```$ cat cluster.txt```\n",
    "    \n",
    "    - These two commands are equivalent:\n",
    "    \n",
    "    ``` $hadoop fs -copyFromLocal -f cluster.txt /ex03/```\n",
    "    \n",
    "     ``` $hadoop fs -put -f cluster.txt /ex03/```\n",
    "     \n",
    "    - The -f option will overwrite the destination if it already exists.\n",
    "\n",
    "8. The reverse operation of ```copyFromLocal```, i.e., download a file from HDFS to the file system where your terminal locates: ```copyToLocal```\n",
    "\n",
    "    - ```$hadoop fs -copyToLocal /example/data/fruits.txt```\n",
    "    - If you do ```$ ls``` in your terminal, you should see the \"fruits\" file there.\n",
    "<br><br>    \n",
    "\n",
    "9. Delete a directory: ```rmdir```\n",
    "    - We cannot delete a non-empty folder directly, so we need to remove the files inside by ```$hadoop fss -rm /ex03/*```\n",
    "    - We then remove the folder ```$hadoop fs -rmdir /ex03```\n",
    "    - If you do ls again on the deleted folder, it should not exist any more. ```$hadoop fs -ls /ex03```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we try to upload a big file (>1G) to our HDFS cluster and you can inspect the cluster with Ambari. \n",
    "\n",
    "Download a compressed dump of all the articles on Wikipedia:\n",
    "```$ wget https://bigdataforeng.blob.core.windows.net/ex05/wiki.tar.gz```\n",
    "\n",
    "Next, let's upload this file from the local file system to HDFS:\n",
    "\n",
    "```$hadoop fs -mkdir /bigdata```\n",
    "\n",
    "```$hadoop fs -put ~/wikibig.tar.gz /bigdata```\n",
    "\n",
    "```$hadoop fs -ls /bigdata```\n",
    "\n",
    "\n",
    "You should see a listing of the following form with details about the uploaded file (permissions, ownership, size and modification date):\n",
    "\n",
    "```Found 1 items\n",
    "-rw-r--r--   1 sshuser supergroup 1116569600 2020-03-01 17:45 /bigdata/wikibig.tar.gz```\n",
    "\n",
    "This was a quick overview of the basic shell commands and in a following exercise we will run a MapReduce computation over data stored in HDFS.\n",
    "\n",
    "Feel free to try out the commands on your own. For instance, pick an image file in your computer (or you can also download a random one) and try to upload it to HDFS. You may need to create an empty directory before uploading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Important:** Don't forget to terminate and delete your cluster once you are done (!)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
