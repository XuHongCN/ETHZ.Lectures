{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk # install nltk by sudo pip -U nltk. nltk is a package for natural language processing.  \n",
    "# nltk.download() # run download \n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the speech.txt contains Mr. Trump's speeches.\n",
    "with open('speech.txt', 'r') as content_file:\n",
    "    corpus_raw = content_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to lower case\n",
    "corpus_raw = corpus_raw.lower()\n",
    "\n",
    "# some preprocessing on the corpus\n",
    "corpus_raw = corpus_raw.replace('.',' . ')\n",
    "corpus_raw = corpus_raw.replace('?',' . ')\n",
    "corpus_raw = corpus_raw.replace(',',' ')\n",
    "corpus_raw = corpus_raw.replace('\\'s','')\n",
    "corpus_raw = corpus_raw.replace('\\'',' ')\n",
    "corpus_raw = corpus_raw.replace(';',' . ')\n",
    "# extracting words \n",
    "words = []\n",
    "for word in corpus_raw.split():\n",
    "    if word != '.': # because we don't want to treat . as a word\n",
    "        words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_tag = nltk.pos_tag(words)\n",
    "words_status = np.zeros(len(words))\n",
    "word2stat = {}\n",
    "for (word, stat) in word_tag: \n",
    "    if (not re.match('.*\\d+', word)) and (stat == 'NN' or stat == 'NNS' or stat == 'JJ') and len(word)>1: \n",
    "        word2stat[word] = 1\n",
    "    else: \n",
    "        word2stat[word] = 0\n",
    "word_sequence = [word for word in words if word2stat[word] == 1]\n",
    "words = set(word_sequence)\n",
    "# indexing the words \n",
    "word2int = {}\n",
    "int2word = {}\n",
    "vocab_size = len(words) # gives the total number of unique words\n",
    "for i,word in enumerate(words):\n",
    "    word2int[word] = i\n",
    "    int2word[i] = word\n",
    "index_sequence = [None]*(len(word_sequence))\n",
    "for i in range(len(word_sequence)): \n",
    "    index_sequence[i] = word2int[word_sequence[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get a list of indeces in a window around an index.\n",
    "def get_target(words, idx, window_size=5):\n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = set(words[start:idx] + words[idx+1:stop+1])\n",
    "    \n",
    "    return list(target_words)    \n",
    "\n",
    "#Create a generator of word batches as a tuple (inputs, targets) \n",
    "def get_batches(words, batch_size, window_size=5):\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedding probablisitc model \n",
    "D = 100 # embedding dimension\n",
    "X = tf.placeholder(tf.int32,[None])\n",
    "Y = tf.placeholder(tf.int32,[None,None])\n",
    "embedding = tf.Variable(tf.random_uniform((vocab_size,D),-1,1)) # word embeddings \n",
    "embed = tf.nn.embedding_lookup(embedding, X) # use tf.nn.embedding_lookup to get the hidden layer output\n",
    "# solution a. \n",
    "softmax_w = tf.Variable(tf.truncated_normal((vocab_size, D)))\n",
    "softmax_b = tf.Variable(tf.zeros(vocab_size))\n",
    "batch_size = 300\n",
    "loss = tf.nn.sampled_softmax_loss(\n",
    "        weights=softmax_w,\n",
    "        biases=softmax_b,\n",
    "        labels=Y,\n",
    "        inputs=embed,\n",
    "        num_sampled=batch_size,\n",
    "        num_classes=vocab_size)\n",
    "rloss = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.AdamOptimizer().minimize(rloss)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training \n",
    "T = 10 \n",
    "epochs = 20\n",
    "window_size = 7\n",
    "for i in range(epochs):\n",
    "#     print('---')\n",
    "    closs = 0 \n",
    "    batches = get_batches(index_sequence, batch_size, window_size)\n",
    "    num = 0 \n",
    "    for x, y in batches:\n",
    "        feed = {X: x,\n",
    "                Y: np.array(y)[:, None]}\n",
    "        # solution b. \n",
    "        train_loss, _ = sess.run([rloss, optimizer], feed_dict=feed)\n",
    "        num +=1\n",
    "        closs += train_loss\n",
    "    closs = closs/float(num)\n",
    "    print('epoch={},loss={}'.format(i,closs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scatter plot the words\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE # for dimensinality reduction\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "normalized_embedding = embedding / norm\n",
    "    \n",
    "embed_mat = sess.run(normalized_embedding)\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embed_mat[500:1000, :]) # embedded words in 2D space\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "for idx in range(embed_tsne.shape[0]):\n",
    "    plt.scatter(embed_tsne[idx, 0],embed_tsne[idx, 1], color='steelblue')\n",
    "    plt.annotate(int2word[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
